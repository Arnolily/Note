{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Daily Life and Thoughts on Science","text":"<p>Welcome to my blog! Here you can find links to all my diaries. The purpose of this blog is to document and share the interesting knowledge I acquired.</p> <ul> <li>My comprehension of Resnet and Batch Norms</li> </ul>"},{"location":"Resnet%20%26%20Batch%20Norms/","title":"My comprehension of Resnet and Batch Norms","text":"<p>Resnet and Batch Normalization has been without doubt two of the most important elements in Deep Learning. This diary is to record my own understanding on why they work.</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#lets-first-talk-about-bn","title":"Let's first talk about BN.","text":""},{"location":"Resnet%20%26%20Batch%20Norms/#improving-numerical-stability","title":"Improving Numerical stability","text":"<p>Suppose we have a 1-layer Linear Neural Network for regression problem, the output  and MSE loss  is:   Through back propagation,   If the net has N layers, then the gradient of the first layer would be , and the product of this sequence would tend to 0 or inf. This is Numerical instability.</p> <p>With BN, we could restric X into any Gaussian distribution, thus fixing the instability of  and .</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#perhaps-not-alleviate-internal-covariate-shift-1","title":"(Perhaps not) Alleviate Internal Covariate Shift [1]","text":"<p>In the original paper of Batch Normalizaion, they defined ICS.</p> <p>ICS is the phenomenon where the distribution of inputs to a layer in the network changes due to an update of parameters of the previous layers.</p> <p>Mathematically, each layer can be derived as:   Where  is the mapping function between input  and output . Basically, they are learning this map between input pattern and output pattern. However, due to ICS, the input pattern could be constantly changing, and such changes could break the learnt mapping function; therefore, ICS is believed to have detrimental effect on training process.</p> <p>However, other researchers had shown that ICS will not effect performance and BN does not reduce ICS. They prefer to explain the effectiveness of BN in another way.</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#smoother-loss-landscape-2","title":"Smoother Loss landscape [2]","text":"<p>These researchers shown that BN contributes to the smoother Loss landscape, which could be visualized: </p> <p>With BN, the Loss landscape is smoother, training efficiency will be improved, and that is why higher learning rate could be utilized. </p>"},{"location":"Resnet%20%26%20Batch%20Norms/#better-use-of-non-linearity-3","title":"Better use of non-linearity [3]","text":"<p>Please be noted, there is a linear transformation (ie. scale and shift) after feature map  is normalized.</p> <p></p> <p>Let us suppose we use sigmoid function as activation function.</p> <p></p> <p>It is pretty clear that for , we have  . Which means the activation function is similar to an identity transformation. That is not a good sign, because the purpose of activation function is to add non-linearity in to the model. Therefore, without scale and shift, the non-linearity will be weak and the fitting process would be hard.</p> <p>Even for ReLU, which seems not going to be heavily effected, also witnessed the situation where all the neurons are either activated  or deactivated . Be aware that if a ReLU neuron is always active, then it is linear; if it is always deactive, then it does not exist.</p> <p>But in my perspective, this might not be the most important contribution of BN. Experiments have proved that if we put BN after activation, network could even sometimes outperform!</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#citation","title":"Citation","text":"<p>[1] Batch normalization: Accelerating deep network training by reducing internal covariate shift. [2] How Does Batch Normalization Help Optimization? [3] The shattered gradients problem: If resnets are the answer, then what is the question?</p>"},{"location":"index%20copy/","title":"My Daily Life and Thoughts on Science","text":"<p>Welcome to my blog! Here you can find links to all my diaries. The purpose of this blog is to document and share the interesting knowledge I acquired.</p> <ul> <li>My comprehension of Resnet and Batch Norms</li> </ul>"}]}