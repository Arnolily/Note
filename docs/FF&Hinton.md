# The Forward-Forward Algorithm: A New Revolution Brought by Geoffrey Hinton  
***At the age of 74, Geoffrey Hinton introduced a new algorithm to replace the backpropagation method—the very technique he popularized in 1986—in an effort to bring deep learning closer to a biologically plausible interpretation. This blog is to show respect for him and discuss my understanding of this work as well as recent developments on this amazing algorithm.***

> And lo, he declared, “Let there be a new way,” and thus the Forward-Forward algorithm was brought forth, that it might supplant the backpropagation he had once bestowed upon the minds of men in the year of 1986. And it was good. ------ *Hinton's Epiphany: A New Era in Deep Learning* (I made this up)

![Hinton](images/hinton.jpeg){: .center style="height:300px;"}

## Prerequisite: Problems with the Forward-Backward Method  
Nowadays, most networks utilize Backpropagation __(BP)__ to optimize the network. This is very effective, but its inherent flaws limit the upper bound of its capability. This inspired Hinton and the community to pursue a new method.

### What is the limitation?

1. **First**, BP uses the chain rule to compute derivatives. This requires the forward computational graph to be precise, with no black-box process allowed.

2. **Second**, the ideal way to train a network should be continuous. For example, a robot should learn the rules of the world as it observes, with each frame and movement being directly processed by its brain. This stream of events should flow uninterrupted. It would seem unnatural if the robot had to stop observing every time it received a new frame to perform the BP process.

3. **Lastly**, and most importantly, BP is not a biologically plausible learning mechanism. In fact, it is highly unlikely that optimization signals are propagated backward through the brain. This suggests that the human brain does not learn using BP. From a practical standpoint, this aligns with common sense. Models like ChatGPT rely on Backpropagation Through Time (BPTT) to adjust their parameters, but this process is clearly unfeasible in the real world—how could a biological system, or any real-world process, literally travel backward through time?

4. Of course, there are many other disadvantages to BP, but understanding that BP is not perfect is enough to move forward.

## A Brief Introduction to the Forward-Forward Algorithm  
The training process of the Forward-Forward Algorithm can be broken down into two processes: positive learning and negative learning. These two processes are executed in an alternating fashion—first positive learning, then negative learning.

### A. Loss  
During positive learning, we aim to lift up the output of each layer. The loss is:  
$$ \mathcal{L}_{pos} = \log (1 + \theta - y^2) $$  
where \(y\) is the output of each layer, and \(\theta\) is a hyper-parameter.

During negative learning, we aim to push down the output of each layer. The loss is:  
$$ \mathcal{L}_{neg} = \log (1 - \theta + y^2) $$

### B. Data  
During positive learning, we input 'fact.' During negative learning, we input 'delusion.'

By 'fact,' we mean the ground truth, while 'delusion' refers to something random or generated by some generative models.

For example, in a simple MNIST dataset classification problem, 'fact' could be the image with the correct label mapping, while 'delusion' would be the image with a random label mapping.

![fact](fact.png){: .center style="height:300px;"}
![delusion](delusion.png){: .center style="height:300px;"}

Another example for unsupervised learning would be 'fact' as the original image and 'delusion' as a combination of two different images.

![](delusion-unsupervised.png){: .center style="height:300px;"}

For RNNs and language models, during the positive training period, inputs will be real text data from the dataset. After the positive training period, the model will generate a piece of text data to serve as input for the negative training period.

### C. Limitations
The original paper practiced this method on MLP only, and when I implemented the supervised method on a CNN, it failed, as expected. The label information, treated as marginal data, was lost during the convolution process. Since the only distinction between positive training and negative learning lies in the mapped label information, losing this information would result in a complete failure of the training process. Therefore, to dive deeper into the potential of the FF algorithm, we need to look for better ways to generate negative data.

## Some latest progress on FF algorithm
### A. Training Convolutional Neural Networks with the Forward-Forward Algorithm
This paper proposed an intuitive method for generating negative samples. They defined different labels as 2-D strips matching the size of the image and mapped them onto the images. Unlike the original 1-D one-hot labels, these 2-D labels are not treated as marginal information, meaning they are retained during convolution. As a result, they can be captured by the CNN model, potentially leading to better performance than the vanilla Forward-Forward (FF) algorithm.

However, the results from my experiments indicate that this method may not be as effective on more complex datasets such as CIFAR-10 and CIFAR-100. Please note that the paper does not provide code, and although not likely, my re-implementation on the CIFAR datasets may contain errors.

### B.Convolutional Channel-wise Competitive Learning for the Forward-Forward Algorithm
This paper re-implemented the FF algorithm, and it changed the way to use label information. Still remember that we want to lift up the output during positive training and push down during negative training? This time, they combined the positive training and negative training. When their model is trained, the output of each layer will be uniformly divide into [num_of_classes] parts, on the channel dimension. Then, the magnitude of the part that aligns with the label will be lifted up, and the other parts will be pushed down. It would be better elaborated in this pseudo code:

    output = layer(input) # dimension = [C, H, W]
    channel_size = output.shape[0] # channel_size = C
    channel_part = torch.split(output, 
    channel_size // num_of_classes, dim = 1) # divide output along the channel dimension

    loss_pos = torch.exp(1 + theta - channel_part[label].pow(2).mean())
    
    loss_net = torch.exp(1 - theta + channel_part[others].pow(2).mean())
This is a very innovative training method to me, and intuitively speaking, it treated every method as a classifier. It could work well on cifar-10 dataset, with an accuracy of 79%.

### Self-Contrastive Forward-Forward Algorithm
