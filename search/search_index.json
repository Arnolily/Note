{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Daily Life and Thoughts on Science","text":"<p>Welcome to my blog! Here you can find links to all my diaries. The purpose of this blog is to document and share the interesting knowledge I acquired.</p> <ul> <li>My comprehension of Resnet and Batch Norms</li> <li>The Forward-Forward Algorithm: A New Revolution Brought by Geoffrey Hinton</li> <li>Feature Learning from Continual Learning</li> </ul>"},{"location":"FF%26Hinton/","title":"The Forward-Forward Algorithm: A New Revolution Brought by Geoffrey Hinton","text":"<p>At the age of 74, Geoffrey Hinton introduced a new algorithm to replace the backpropagation method\u2014the very technique he popularized in 1986\u2014in an effort to bring deep learning closer to a biologically plausible interpretation. This blog is to show respect for him and discuss my understanding of this work as well as recent developments on this amazing algorithm.</p> <p>And lo, he declared, \u201cLet there be a new way,\u201d and thus the Forward-Forward algorithm was brought forth, that it might supplant the backpropagation he had once bestowed upon the minds of men in the year of 1986. And it was good. ------ Hinton's Epiphany: A New Era in Deep Learning (I made this up)</p> <p></p>"},{"location":"FF%26Hinton/#prerequisite-problems-with-the-forward-backward-method","title":"Prerequisite: Problems with the Forward-Backward Method","text":"<p>Nowadays, most networks utilize Backpropagation (BP) to optimize the network. This is very effective, but its inherent flaws limit the upper bound of its capability. This inspired Hinton and the community to pursue a new method.</p>"},{"location":"FF%26Hinton/#what-is-the-limitation","title":"What is the limitation?","text":"<ol> <li> <p>First, BP uses the chain rule to compute derivatives. This requires the forward computational graph to be precise, with no black-box process allowed.</p> </li> <li> <p>Second, the ideal way to train a network should be continuous. For example, a robot should learn the rules of the world as it observes, with each frame and movement being directly processed by its brain. This stream of events should flow uninterrupted. It would seem unnatural if the robot had to stop observing every time it received a new frame to perform the BP process.</p> </li> <li> <p>Last, and most importantly, BP is not a biologically plausible learning mechanism. In fact, it is highly unlikely that optimization signals are propagated backward through the brain. This suggests that the human brain does not learn using BP. From a practical standpoint, this aligns with common sense. Models like ChatGPT rely on Backpropagation Through Time (BPTT) to adjust their parameters, but this process is clearly unfeasible in the real world\u2014how could a biological system, or any real-world process, literally travel backward through time?</p> </li> <li> <p>Of course, there are many other disadvantages to BP, but understanding that BP is not perfect is enough to continue reading.</p> </li> </ol>"},{"location":"FF%26Hinton/#a-brief-introduction-to-the-forward-forward-algorithm","title":"A Brief Introduction to the Forward-Forward Algorithm","text":"<p>The training process of the Forward-Forward Algorithm can be broken down into two processes: positive learning and negative learning. These two processes are executed in an alternating fashion\u2014first positive learning, then negative learning, then positive again, and then negative... Goodness is defined as the mean square of the output of each layer.</p>"},{"location":"FF%26Hinton/#a-loss","title":"A. Loss","text":"<p>During positive learning, we aim to lift up the goodness(output of each layer). The loss is:  where  is the output of each layer, and  is a hyper-parameter.</p> <p>During negative learning, we aim to push down the goodness. The loss is: </p>"},{"location":"FF%26Hinton/#b-data","title":"B. Data","text":"<p>During positive learning, we input 'fact.' During negative learning, we input 'delusion.'</p> <p>By 'fact,' we mean the ground truth, while 'delusion' refers to something random or generated by some generative models.</p> <p>For example, in a simple MNIST dataset classification problem, labels will be mapped to the first 10 pixels on the image. 'Fact' could be the image with the correct label mapping, while 'delusion' would be the image with a random label mapping.</p> <p> </p> <p>Another example for unsupervised learning would be 'fact' as the original image and 'delusion' as a combination of two different images.</p> <p></p> <p>For RNNs and language models, during the positive training period, inputs will be real text data from the dataset. After the positive training period, the model will generate a piece of text data to serve as input for the negative training period.</p>"},{"location":"FF%26Hinton/#c-inference","title":"C. Inference","text":"<p>During the inference process, the network will concatenate all the goodnesses from each layer, and choose the prediction with the biggest goodness.</p>"},{"location":"FF%26Hinton/#d-biological-inspiration","title":"D. Biological Inspiration","text":"<p>One of the fascinating aspects of Biologically Inspired Intelligence is that when we design networks based on biological principles, these networks often display phenomena that can help us better understand brain mechanisms, and FF could be one of them. Hinton pointed out, the training process of FF could be similar to how a infant understand the world. When an infant is awake, they observe the real world, where everything they see is treated as \"ground truth\"\u2014it's simply what is. Then, when he is asleep, his brain would attempts to replay and make sense of what it has learned. That could be in the form of dreams. In this analogy, the awake phrase would be positive training process, where the brain (or network) absorbs and experiences the world; and the asleep phrase would be negative training process, where the brain refines its understanding by processing and adjusting based on the knowledge it has gathered. </p> <p>Indeed, this is a fascinating analogy, but ultimately, it is up to each individual to decide whether they will buy this story.</p>"},{"location":"FF%26Hinton/#e-limitations","title":"E. Limitations","text":"<p>The original paper practiced this method on MLP only, and when I implemented the supervised method on a CNN, it failed, as expected. The label information, treated as marginal data, was lost during the convolution process. Since the only distinction between positive training and negative learning lies in the mapped label information, losing this information would result in a complete failure of the training process. Therefore, to dive deeper into the potential of the FF algorithm, we need to look for better ways to generate negative data.</p>"},{"location":"FF%26Hinton/#some-latest-progress-on-ff-algorithm","title":"Some latest progress on FF algorithm","text":""},{"location":"FF%26Hinton/#a-training-convolutional-neural-networks-with-the-forward-forward-algorithm","title":"A. Training Convolutional Neural Networks with the Forward-Forward Algorithm","text":"<p>This paper proposed an intuitive method for generating negative samples. They defined different labels as 2-D strips matching the size of the image and mapped them onto the images. Unlike the original 1-D one-hot labels, these 2-D labels are not treated as marginal information, meaning they are retained during convolution. As a result, they can be captured by the CNN model, potentially leading to better performance than the vanilla Forward-Forward (FF) algorithm.</p> <p>However, the results from my experiments indicate that this method may not be as effective on more complex datasets such as CIFAR-10 and CIFAR-100. Please note that the paper does not provide code, and it is possible, although not likely, that my re-implementation on the CIFAR datasets contain errors.</p> <p></p>"},{"location":"FF%26Hinton/#bconvolutional-channel-wise-competitive-learning-for-the-forward-forward-algorithm","title":"B.Convolutional Channel-wise Competitive Learning for the Forward-Forward Algorithm","text":"<p>This paper re-implemented the FF algorithm, and it changed the way to use label information. Still remember that we want to lift up the output during positive training and push down during negative training? This time, they combined the positive training and negative training. When their model is trained, the output of each layer will be uniformly divide into [num_of_classes] parts, on the channel dimension. Then, the magnitude of the part that aligns with the label will be lifted up, and the other parts will be pushed down. It would be better elaborated in this pseudo code:</p> <pre><code>output = layer(input) # dimension = [C, H, W]\nchannel_size = output.shape[0] # channel_size = C\nchannel_part = torch.split(output, \nchannel_size // num_of_classes, dim = 1) # divide output along the channel dimension\n\nloss_pos = torch.exp(1 + theta - channel_part[label].pow(2).mean())\n\nloss_net = torch.exp(1 - theta + channel_part[others].pow(2).mean())\n</code></pre> <p>This is a very innovative training method to me, and intuitively speaking, it treated every method as a classifier. It could work well on cifar-10 dataset, with an accuracy of 79%.</p> <p>Now this method does have a big problem, is that its capability is highly limited. It can only deal with image classification problem, with a limited number of classes. Since its training method is highly correlated with labels, some learning scenarios such as continual learning will no longer be applicable. Also, this method completely destroyed the feature learning behavior of models, which is destructive to the biological plausibility of neural network.</p> <p></p>"},{"location":"FF%26Hinton/#c-self-contrastive-forward-forward-algorithm","title":"C. Self-Contrastive Forward-Forward Algorithm","text":"<p>I believe this figure could do serve as an introduction. This method involves simply inputting two images and maximizing goodness when two images are the same, and minimizing goodness when they are not the same.</p> <p>The paper is still in preprint version and no code is provided, and I failed to reimplement their method. So I will make no comment about this idea.</p>"},{"location":"FF%26Hinton/#references","title":"References","text":"<p>The Forward-Forward Algorithm: Some Preliminary Investigations</p> <p>Training Convolutional Neural Networks with the Forward-Forward Algorithm</p> <p>Convolutional Channel-wise Competitive Learning for the Forward-Forward Algorithm</p> <p>Self-Contrastive Forward-Forward Algorithm</p> <p>Geoffrey Hinton Unpacks The Forward-Forward Algorithm [Youtube Video at: URL]</p>"},{"location":"Feature_Learning/","title":"Feature Learning from Continual Learning","text":"<p>Continual Learning: learning to remember.</p> <p>Feature learning: learning to understand.</p> <p>I have always been curious about one thing: what makes neural networks so powerful? One of the sub-questions is: what do neural networks actually learn?</p> <p>A very famous example I would like to use is self-attention: </p> <p>When generating words, the model concentrates on different parts of the picture, similar to how human would focus on.</p> <p>But are all models learned in this way?</p> <p>First, we need to remember what the models are doing. They are extracting features, discarding unneeded information (such as noise), and focusing on the valuable content. This valuable content is then fed into the final fully connected (fc) layer, which acts as a classifier\u2014the layer that learns the mapping between features and labels.</p>"},{"location":"Feature_Learning/#an-interesting-phenomenon","title":"An Interesting Phenomenon","text":"<p>I was enrolled in an AI-related course at my school, and one of the projects was to do image classification on a dataset we collected ourselves. The dataset was extremely poor, and the data distribution was highly unideal. For example, a tin can could have a label of 'metal', 'waste', or even 'hazardous waste' if it contains outdated medicine. Let\u2019s call it the UESTC dataset.</p> <p>It seemed almost impossible for models to achieve good accuracy on such a dataset. Initially, I blamed the UESTC dataset and set it aside. Days later, a friend of mine told me he achieved a high accuracy of 80% using a pretrained model on ImageNet. This immediately drew my attention. By using a simple transfer-learning technique, the model could learn to classify the UESTC dataset with high accuracy. Surely, the pretraining experience on ImageNet helped the model learn to extract more general information, instead of focusing on minor details that were highly correlated with a specific dataset.</p> <p>Now, I would describe this phenomenon as feature learning. In fact, I believe one of the most important aspects of transfer learning is feature learning. The teacher model is pretrained on a high-quality dataset, where it learns to extract features. These pretrained parameters can then be transferred to a downstream task as initialization. This starting point of gradient descent significantly influences the entire process and most likely leads to better performance.</p> <p>So naturally, I thought about training the model on the UESTC dataset to improve its generalization, hoping it might then perform better on ImageNet. However, I tried and failed. The model trained on the UESTC dataset forgot about ImageNet, resulting in overfitting on the UESTC data.</p>"},{"location":"Feature_Learning/#continual-learning","title":"Continual Learning","text":"<p>As we can see, this method only improves accuracy on a specific dataset. Once we start training the model on the downstream task, the parameter space begins to shift, and it loses its generalization ability. Like this graph:</p> <p></p> <p>What if we want this model to learn and remember? Is it possible to keep the parameters in the \"sweet spot,\" where the model can balance the trade-off between performance and overfitting? It is possible, and this problem is known as the continual learning problem.</p> <p>Continual learning techniques can help alleviate the phenomenon of catastrophic forgetting, which refers to the model forgetting previous tasks when training on later tasks.</p> <p>Another concept, though related, is called Lifelong Learning. This aims to achieve a learning process similar to that of humans\u2014learning forever while continuously remembering the most important information.</p> <p>Below, I will briefly introduce some popular continual learning techniques.</p> <ol> <li> <p>Adding Constraints: such as EWC, OWM, GPM</p> <p>These techniques add constraints to the loss function. The constraints can be related to the mean, gradient, or variance of parameters. The goal is to create a gradient map where the minima lie in the sweet spot.</p> </li> <li> <p>Dividing Models: such as HAT</p> <p>This is a tricky but unsustainable technique. It divides a model into different parts, with each part responsible for a specific task. However, this does not suit the goal of lifelong learning, as the model will quickly run out of free space. I also want to emphasize that short-term continual learning is easy but unimpressive. \ud83d\udccc I will explain why below.</p> </li> <li> <p>Memory Pool: such as A-GEM</p> <p>This method requires an additional memory pool to store previous gradients or data. Occasionally, the model revisits the memory pool and attempts to \"review\" previous knowledge.</p> </li> </ol>"},{"location":"Feature_Learning/#surprising-performance-of-feature-learning","title":"Surprising performance of feature learning","text":"<p>What attracts me to continual learning is its motivation\u2014i.e., learning to understand. To prevent catastrophic forgetting, the ideal approach is to learn the general rules of data, without over-concentrating on minor details. Therefore, I believe feature learning is one of the paths to continual learning.</p> <p>However, the techniques I\u2019ve presented above do not consider data distillation at all. There is nothing about feature extraction or generalization. They focus primarily on reducing conflict during each training process. It\u2019s like a double-edged sword. Perhaps in some scenarios, these techniques may work better, but at least for their baselines (which are typically tested on split-CIFAR100 or P-MNIST), they are somewhat less effective. Why do I say this? Because traditional backpropagation (BP) can already do just fine.</p> <p></p> <p>BP* means to load the model with the highest validation accuracy for each task.</p> <p>BP** means to freeze the feature extractor (i.e., the parameters before the classifier).</p> <p>Does this result surprise you? Well, the result is tested across 5 seeds, and they should be enough to draw meaningful conclusions. Although BP is highly unstable in continual learning (and in some seeds, it could forget everything\u2014of course, I didn\u2019t use those seeds for the baseline), some of its transformations achieve impressive accuracy.</p> <p>Why? How can BP** achieve such a high score? The answer is simple: because fundamentally, BP** is not continual learning. For each task, we are only training a single fully connected (fc) layer, as the previous parameters are frozen.</p> <p>\ud83d\udccc: This is why I say there is little significance in achieving short-term continual learning, because BP can already do it very well\u2014sometimes even better than techniques such as HAT.</p> <p>This holds true only for task increments. In the case of class increments, BP fails entirely, as the classifier becomes the bottleneck. However, the fundamental concept of feature learning stays unchanged.</p> <p>Even for dataset such as permuted-mnist, which was permuted randomly and contain no general features, BP** still performs with outstanding accuracy. The feature extractor learnt to increase the rank of the feature matrix, leading to a bigger parameter space which contains more information.</p>"},{"location":"Feature_Learning/#a-remarkable-but-flawed-method-hat","title":"A Remarkable but Flawed Method: HAT","text":"<p>When I first implemented HAT, its performance left me completely astonished. It simply does not forget.</p> <p>However, this comes at a cost. One peculiar issue is its incompatibility with BatchNorm, along with a strict learning rate limitation of 5e-2. A standard toy model that performs well with other techniques must be specifically adjusted to work with HAT. The incompatibility with BatchNorm significantly restricts HAT\u2019s ability to scale to larger models due to potential numerical instability.</p> <p>But that\u2019s not the only drawback. A fundamental issue with HAT is its architecture, which essentially partitions the model into num_classes segments. When making a prediction, the model activates only a specific portion of itself. While this mechanism effectively prevents forgetting, it raises concerns when handling a large number of tasks\u2014or even an infinite number, as defined in lifelong learning. In practice, our toy model failed on split-Tiny-ImageNet when using HAT, despite performing flawlessly with other methods.</p>"},{"location":"Feature_Learning/#conclusion","title":"Conclusion","text":"<p>I believe a promising direction for continual learning should align with human learning principles\u2014focusing on understanding rather than mere memorization. In other words, leveraging feature learning to achieve continual learning would be a far more effective approach.</p>"},{"location":"Resnet%20%26%20Batch%20Norms/","title":"My comprehension of Resnet and Batch Norms","text":"<p>Skip-connections and rescaling (including normalization) appear in most neural networks. However, some papers use these techniques inappropriately, causing unnecessary network complexity. It is therefore important to revisit the basics for both experimental inspiration and theoretical understanding.</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#possible-advantages-of-batch-normalization-bn","title":"Possible Advantages of Batch Normalization (BN)","text":""},{"location":"Resnet%20%26%20Batch%20Norms/#improving-numerical-stability","title":"Improving Numerical Stability","text":"<p>Suppose we have a 1-layer linear neural network for a regression problem. The output  and the Mean Squared Error (MSE) loss  are:</p> <p> </p> <p>Through backpropagation, the gradient  is:</p> <p> </p> <p>If the network has  layers, then the gradient of the first layer would be , and the product of this sequence would tend to 0 or infinity. This is numerical instability.</p> <p>With BN, we can restrict  to any Gaussian distribution, thus mitigating the instability of  and .</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#perhaps-not-alleviating-internal-covariate-shift-1","title":"(Perhaps Not) Alleviating Internal Covariate Shift [1]","text":"<p>In the original paper on Batch Normalization, they defined Internal Covariate Shift (ICS).</p> <p>ICS is the phenomenon where the distribution of inputs to a layer in the network changes due to an update of parameters in previous layers.</p> <p>Mathematically, each layer can be derived as:</p> <p> </p> <p>where  is the mapping function between input  and output . Essentially, layers are learning this map between input patterns and output patterns. However, due to ICS, the input pattern could constantly change, which might disrupt the learned mapping function. Therefore, ICS is believed to have a detrimental effect on the training process.</p> <p>However, I propose an alternative understanding of ICS. In traditional machine learning, we assume that data are IID (i.e., independently and identically distributed). This is the foundation of machine learning and the prerequisite of MLE (i.e., Maximum Likelihood Estimation), which is the original form of all loss functions. By using BN, we can ensure the second I in IID, which is identically distributed. This could lead to better generalization performance for the network.</p> <p>Other researchers, however, have shown that ICS does not affect performance and that BN does not reduce ICS. They prefer to explain BN\u2019s effectiveness differently.</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#smoother-loss-landscape-2","title":"Smoother Loss Landscape [2]","text":"<p>These researchers demonstrated that BN contributes to a smoother loss landscape, which can be visualized:</p> <p></p> <p>With BN, the loss landscape is smoother, which improves training efficiency and allows for a higher learning rate.</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#better-use-of-non-linearity-3","title":"Better Use of Non-linearity [3]","text":"<p>Note that there is a linear transformation (i.e., scaling and shifting) applied after the feature map  is normalized.</p> <p></p> <p>Suppose we use the sigmoid function as the activation function.</p> <p></p> <p>It\u2019s clear that for , we have . This means the activation function acts almost as an identity transformation, which is not ideal because the purpose of an activation function is to add non-linearity to the model. Therefore, without scaling and shifting, the non-linearity would be weak, making the fitting process difficult.</p> <p>Even for ReLU, which seems less affected, there are cases where all neurons are either activated  or deactivated . If a ReLU neuron is always active, then it is linear; if it\u2019s always inactive, it doesn\u2019t contribute to the network.</p> <p>From my perspective, however, this might not be BN\u2019s most important contribution. Experiments have shown that placing BN after the activation function can sometimes improve network performance!</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#possible-advantages-of-skip-connections","title":"Possible Advantages of Skip-Connections","text":""},{"location":"Resnet%20%26%20Batch%20Norms/#smoother-gradient-flow","title":"Smoother Gradient Flow","text":"<p>As pointed out in [4], authored by the ResNet creator He, one benefit of skip-connections is smoother gradient flow. For each skip layer, the output can be derived as:</p> <p> </p> <p>And the gradient is:</p> <p> </p> <p>With ResNet, the gradient will be a sum of \u201cgeometric equations,\u201d preventing it from vanishing.</p>"},{"location":"Resnet%20%26%20Batch%20Norms/#solving-the-network-degradation-problem","title":"Solving the Network Degradation Problem","text":"<p>In the original ResNet paper, He claimed that ResNet solved the \"network degradation problem.\" The idea was that if a deeper model performs worse than a shallower one on the test set, it implies that the additional layers in the deeper model are less effective than a straightforward identity mapping. If those extra layers simply acted as identity mappings, the deeper model would match the performance of the shallower model.</p> <p>With this insight, he designed skip-connections, which are essentially manually added identity mappings.</p> <p>The real question is, what causes network degradation? Is the degradation problem truly due to a failure to achieve identity mapping? This is an area where researchers differ, and I will present some interesting studies on the topic.</p> <p>Gradient Correlation [2]</p> <p>Balduzzi et al. [2] proposed a new explanation for network degradation. They first suggested that ResNet alleviates the shattered gradient problem. The shattered gradient problem is defined as follows:</p> <p>Shattered gradients undermine the effectiveness of algorithms that assume gradients at nearby points are similar, such as momentum-based and accelerated methods.</p> <p>Under this assumption, they quantified the shattered gradient problem as the autocorrelation function (ACF). A higher ACF leads to higher performance and a reduced shattered gradient problem. They demonstrated, through both mathematical proofs and experiments, that skip connections enhance ACF. However, their proof relied on very strong assumptions and was not entirely convincing to me.</p> <p></p>"},{"location":"Resnet%20%26%20Batch%20Norms/#citation","title":"Citation","text":"<p>[1] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. [2] How Does Batch Normalization Help Optimization? [3] The Shattered Gradients Problem: If ResNets Are the Answer, Then What Is the Question? [4] Identity Mappings in Deep Residual Networks.</p>"}]}